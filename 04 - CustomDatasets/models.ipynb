{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850082b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ab2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196f4dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_transform_trivial_augment = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb6bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "venv_dir = Path(sys.prefix)\n",
    "project_root = venv_dir.parent\n",
    "image_path = project_root/\"data/pizza_steak_sushi\"\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\n",
    "test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n",
    "train_data_augmented = datasets.ImageFolder(root=train_dir, transform=train_transform_trivial_augment)\n",
    "\n",
    "train_dataloader_simple = DataLoader(\n",
    "    dataset=train_data_simple,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader_simple = DataLoader(\n",
    "    dataset=test_data_simple,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_dataloader_augmented = DataLoader(\n",
    "    dataset=train_data_augmented,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "142cb74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyVGG                                  [1, 3]                    --\n",
       "├─Sequential: 1-1                        [1, 10, 32, 32]           --\n",
       "│    └─Conv2d: 2-1                       [1, 10, 64, 64]           280\n",
       "│    └─ReLU: 2-2                         [1, 10, 64, 64]           --\n",
       "│    └─Conv2d: 2-3                       [1, 10, 64, 64]           910\n",
       "│    └─ReLU: 2-4                         [1, 10, 64, 64]           --\n",
       "│    └─MaxPool2d: 2-5                    [1, 10, 32, 32]           --\n",
       "├─Sequential: 1-2                        [1, 10, 16, 16]           --\n",
       "│    └─Conv2d: 2-6                       [1, 10, 32, 32]           910\n",
       "│    └─ReLU: 2-7                         [1, 10, 32, 32]           --\n",
       "│    └─Conv2d: 2-8                       [1, 10, 32, 32]           910\n",
       "│    └─ReLU: 2-9                         [1, 10, 32, 32]           --\n",
       "│    └─MaxPool2d: 2-10                   [1, 10, 16, 16]           --\n",
       "├─Sequential: 1-3                        [1, 3]                    --\n",
       "│    └─Flatten: 2-11                     [1, 2560]                 --\n",
       "│    └─Linear: 2-12                      [1, 3]                    7,683\n",
       "==========================================================================================\n",
       "Total params: 10,693\n",
       "Trainable params: 10,693\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 6.75\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.82\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 0.91\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2\n",
    "            )\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2\n",
    "            )\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*16*16, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Apparently this leverages the benefits of operator fusion, just seemed smarter\n",
    "        return self.classifier(self.conv_block_2(self.conv_block_1(x)))\n",
    "    \n",
    "modelV0 = TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(train_data_simple.classes)\n",
    ").to(DEVICE)\n",
    "\n",
    "summary(modelV0, input_size=[1, 3, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "983af8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for (X, y) in iter(dataloader):\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    train_acc /= len(dataloader)\n",
    "    train_loss /= len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for (X, y) in iter(dataloader):\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            test_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    test_acc /= len(dataloader)\n",
    "    test_loss /= len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, \n",
    "          loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer,\n",
    "          epochs = 5):\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6073279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354144fd9d7c498489a656933cd367a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.1034 | train_acc: 0.2812 | test_loss: 1.0883 | test_acc: 0.5417\n",
      "Epoch: 2 | train_loss: 1.0996 | train_acc: 0.2812 | test_loss: 1.0889 | test_acc: 0.5417\n",
      "Epoch: 3 | train_loss: 1.1027 | train_acc: 0.2812 | test_loss: 1.0899 | test_acc: 0.5417\n",
      "Epoch: 4 | train_loss: 1.0990 | train_acc: 0.2812 | test_loss: 1.0906 | test_acc: 0.5417\n",
      "Epoch: 5 | train_loss: 1.0966 | train_acc: 0.4023 | test_loss: 1.0898 | test_acc: 0.5417\n",
      "Epoch: 6 | train_loss: 1.1023 | train_acc: 0.2812 | test_loss: 1.0907 | test_acc: 0.5417\n",
      "Total training time: 5.467 seconds\n"
     ]
    }
   ],
   "source": [
    "modelV0 = TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(train_data_simple.classes)\n",
    ").to(DEVICE)\n",
    "\n",
    "NUM_EPOCHS = 6\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=modelV0.parameters(), lr=0.001)\n",
    "\n",
    "start_time = timer()\n",
    "modelV0_results = train(modelV0, train_dataloader_simple, test_dataloader_simple, loss_fn, optimizer, NUM_EPOCHS)\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f977ac",
   "metadata": {},
   "source": [
    "A model is overfitting when the training loss is far lower that the test loss, meaning that it's learning patterns that are too specific and not translating to the test data.\n",
    "A common technique to prevent overfitting is regularization, we don't to make the model more regular, as in capable of dealing with more kinds of data.\n",
    "There are many ways to go about this:\n",
    "1) Get more data\n",
    "2) Simplify the model\n",
    "3) Use data augmentation to make the data harder to learn and also add variety artificially\n",
    "4) Use transfer learning to start with a model already trained for a general task, like recognizing images, then tuning it for our specific task\n",
    "5) Use dropout layers, that randomly remove connections between hidden layers, simplifying the network and making the remaining connections better\n",
    "6) Use learning rate decay, slowly decreasing the learning rate the closer we get to convergence\n",
    "7) Use early stopping to interrupt the training after a model's loss has stopped decreasing, effectively preventing overfitting\n",
    "\n",
    "On the other hand, a model is underfitting if the training and test loss are too high. To increase the model predictive power:\n",
    "1) Add more layers/units to the model\n",
    "2) Tweak the learning rate\n",
    "3) Use transfer learning\n",
    "4) Train for longer\n",
    "5) Use less regularization\n",
    "\n",
    "There is a fine line between underfitting and overfitting, the model needs to learn the data but not too well. Transfer learning is one of the most powerful techniques when it comes to dealing with both underfitting and overfitting. Currently, our model seems to be underfitting, as the final training loss is really high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb0c8425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3283782abd9e49ea88f76e7b5ac44d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0999 | train_acc: 0.2930 | test_loss: 1.1025 | test_acc: 0.1979\n",
      "Epoch: 2 | train_loss: 1.1007 | train_acc: 0.2930 | test_loss: 1.1027 | test_acc: 0.1979\n",
      "Epoch: 3 | train_loss: 1.1002 | train_acc: 0.2930 | test_loss: 1.1029 | test_acc: 0.1979\n",
      "Epoch: 4 | train_loss: 1.0965 | train_acc: 0.4141 | test_loss: 1.1033 | test_acc: 0.1979\n",
      "Epoch: 5 | train_loss: 1.0961 | train_acc: 0.4141 | test_loss: 1.1036 | test_acc: 0.1979\n",
      "Epoch: 6 | train_loss: 1.1002 | train_acc: 0.2930 | test_loss: 1.1038 | test_acc: 0.1979\n",
      "Epoch: 7 | train_loss: 1.0958 | train_acc: 0.4141 | test_loss: 1.1042 | test_acc: 0.1979\n",
      "Epoch: 8 | train_loss: 1.0958 | train_acc: 0.4141 | test_loss: 1.1046 | test_acc: 0.1979\n",
      "Epoch: 9 | train_loss: 1.1005 | train_acc: 0.2930 | test_loss: 1.1041 | test_acc: 0.1979\n",
      "Epoch: 10 | train_loss: 1.1000 | train_acc: 0.2930 | test_loss: 1.1043 | test_acc: 0.1979\n",
      "Epoch: 11 | train_loss: 1.1003 | train_acc: 0.2930 | test_loss: 1.1038 | test_acc: 0.1979\n",
      "Epoch: 12 | train_loss: 1.0960 | train_acc: 0.4141 | test_loss: 1.1042 | test_acc: 0.1979\n",
      "Epoch: 13 | train_loss: 1.0997 | train_acc: 0.2930 | test_loss: 1.1043 | test_acc: 0.1979\n",
      "Epoch: 14 | train_loss: 1.0998 | train_acc: 0.2930 | test_loss: 1.1045 | test_acc: 0.1979\n",
      "Epoch: 15 | train_loss: 1.0958 | train_acc: 0.4141 | test_loss: 1.1049 | test_acc: 0.1979\n",
      "Epoch: 16 | train_loss: 1.1006 | train_acc: 0.2930 | test_loss: 1.1044 | test_acc: 0.1979\n",
      "Epoch: 17 | train_loss: 1.0997 | train_acc: 0.2930 | test_loss: 1.1046 | test_acc: 0.1979\n",
      "Epoch: 18 | train_loss: 1.1005 | train_acc: 0.2930 | test_loss: 1.1041 | test_acc: 0.1979\n",
      "Epoch: 19 | train_loss: 1.0994 | train_acc: 0.2930 | test_loss: 1.1043 | test_acc: 0.1979\n",
      "Epoch: 20 | train_loss: 1.0992 | train_acc: 0.2930 | test_loss: 1.1045 | test_acc: 0.1979\n",
      "Total training time: 18.138 seconds\n"
     ]
    }
   ],
   "source": [
    "modelV1 = TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(train_data_augmented.classes)\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=modelV1.parameters(), lr=0.001)\n",
    "\n",
    "start_time = timer()\n",
    "modelV1_results = train(modelV1, train_dataloader_augmented, test_dataloader_simple, loss_fn, optimizer, NUM_EPOCHS)\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
