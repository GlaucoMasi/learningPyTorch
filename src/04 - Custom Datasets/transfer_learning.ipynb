{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8301a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, datasets\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fdfbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count() or 1\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9848fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing to the 224x224 standard and using normalization \n",
    "\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_transform_trivial_augment = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82003e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "venv_dir = Path(sys.prefix)\n",
    "project_root = venv_dir.parent\n",
    "image_path = project_root/\"data/food-101-pizzasteaksushi/images\"\n",
    "\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=image_path,\n",
    "    transform=train_transform_trivial_augment,\n",
    ")\n",
    "\n",
    "train_size = int(0.8*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset=dataset, lengths=[train_size, len(dataset)-train_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a17377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for (X, y) in iter(dataloader):\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    train_acc /= len(dataloader)\n",
    "    train_loss /= len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for (X, y) in iter(dataloader):\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            test_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    test_acc /= len(dataloader)\n",
    "    test_loss /= len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, \n",
    "          loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, scheduler=None,\n",
    "          epochs=5):\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece9240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "OptimizedModule                               --\n",
      "├─ResNet: 1-1                                 --\n",
      "│    └─Conv2d: 2-1                            9,408\n",
      "│    └─BatchNorm2d: 2-2                       128\n",
      "│    └─ReLU: 2-3                              --\n",
      "│    └─MaxPool2d: 2-4                         --\n",
      "│    └─Sequential: 2-5                        --\n",
      "│    │    └─BasicBlock: 3-1                   73,984\n",
      "│    │    └─BasicBlock: 3-2                   73,984\n",
      "│    └─Sequential: 2-6                        --\n",
      "│    │    └─BasicBlock: 3-3                   230,144\n",
      "│    │    └─BasicBlock: 3-4                   295,424\n",
      "│    └─Sequential: 2-7                        --\n",
      "│    │    └─BasicBlock: 3-5                   919,040\n",
      "│    │    └─BasicBlock: 3-6                   1,180,672\n",
      "│    └─Sequential: 2-8                        --\n",
      "│    │    └─BasicBlock: 3-7                   3,673,088\n",
      "│    │    └─BasicBlock: 3-8                   4,720,640\n",
      "│    └─AdaptiveAvgPool2d: 2-9                 --\n",
      "│    └─Linear: 2-10                           1,539\n",
      "======================================================================\n",
      "Total params: 11,178,051\n",
      "Trainable params: 11,178,051\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f7015ed2df40f58f2088d736aff810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0392 | train_acc: 0.4658 | test_loss: 0.9473 | test_acc: 0.5811\n",
      "Epoch: 2 | train_loss: 0.8841 | train_acc: 0.6496 | test_loss: 0.8344 | test_acc: 0.6743\n",
      "Epoch: 3 | train_loss: 0.7943 | train_acc: 0.7217 | test_loss: 0.7331 | test_acc: 0.7593\n",
      "Epoch: 4 | train_loss: 0.7112 | train_acc: 0.7692 | test_loss: 0.6775 | test_acc: 0.7933\n",
      "Epoch: 5 | train_loss: 0.6603 | train_acc: 0.7854 | test_loss: 0.6329 | test_acc: 0.7993\n",
      "Epoch: 6 | train_loss: 0.6113 | train_acc: 0.8129 | test_loss: 0.5944 | test_acc: 0.8158\n",
      "Epoch: 7 | train_loss: 0.5867 | train_acc: 0.8054 | test_loss: 0.5648 | test_acc: 0.8279\n",
      "Epoch: 8 | train_loss: 0.5620 | train_acc: 0.8179 | test_loss: 0.5523 | test_acc: 0.8262\n",
      "Epoch: 9 | train_loss: 0.5263 | train_acc: 0.8317 | test_loss: 0.5265 | test_acc: 0.8300\n",
      "Epoch: 10 | train_loss: 0.5173 | train_acc: 0.8333 | test_loss: 0.5006 | test_acc: 0.8503\n",
      "Epoch: 11 | train_loss: 0.5058 | train_acc: 0.8267 | test_loss: 0.4822 | test_acc: 0.8531\n",
      "Epoch: 12 | train_loss: 0.4911 | train_acc: 0.8413 | test_loss: 0.4869 | test_acc: 0.8536\n",
      "Epoch: 13 | train_loss: 0.4746 | train_acc: 0.8442 | test_loss: 0.4602 | test_acc: 0.8613\n",
      "Epoch: 14 | train_loss: 0.4647 | train_acc: 0.8496 | test_loss: 0.4480 | test_acc: 0.8498\n",
      "Epoch: 15 | train_loss: 0.4489 | train_acc: 0.8575 | test_loss: 0.4459 | test_acc: 0.8564\n",
      "Epoch: 16 | train_loss: 0.4391 | train_acc: 0.8596 | test_loss: 0.4595 | test_acc: 0.8355\n",
      "Epoch: 17 | train_loss: 0.4412 | train_acc: 0.8538 | test_loss: 0.4455 | test_acc: 0.8295\n",
      "Epoch: 18 | train_loss: 0.4187 | train_acc: 0.8617 | test_loss: 0.4188 | test_acc: 0.8564\n",
      "Epoch: 19 | train_loss: 0.4112 | train_acc: 0.8700 | test_loss: 0.4266 | test_acc: 0.8465\n",
      "Epoch: 20 | train_loss: 0.4183 | train_acc: 0.8596 | test_loss: 0.3926 | test_acc: 0.8509\n",
      "Total training time: 347.959 seconds\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model = torch.compile(model)\n",
    "\n",
    "# resnet18 is pretrained on 1000 classes, we will modify is final fully connected layer for 3 classes\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 3)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(summary(model))\n",
    "\n",
    "# Freezing base layers parameters, to only train the final layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.001)\n",
    "\n",
    "start_time = timer()\n",
    "model_results = train(model, train_dataloader, test_dataloader, loss_fn, optimizer, epochs=NUM_EPOCHS)\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607d643",
   "metadata": {},
   "source": [
    "Model achieves 85% accuracy, I will now do some fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23a670",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Unfreezing the last ResNet block, lowering learning rate and giving the optimizer only the unfrozen parameters with separate learning rates\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m.named_parameters():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlayer4\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfc\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[32m      5\u001b[39m         param.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Unfreezing the last ResNet block, lowering learning rate and giving the optimizer only the unfrozen parameters with separate learning rates\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.layer4.parameters(), 'lr': 0.0001},\n",
    "    {'params': model.fc.parameters(), 'lr': 0.001}\n",
    "], momentum=0.9)\n",
    "\n",
    "\n",
    "# Improving data augmentation\n",
    "better_train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "venv_dir = Path(sys.prefix)\n",
    "project_root = venv_dir.parent\n",
    "image_path = project_root/\"data/food-101-pizzasteaksushi/images\"\n",
    "\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=image_path,\n",
    "    transform=better_train_transform,\n",
    ")\n",
    "\n",
    "train_size = int(0.8*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset=dataset, lengths=[train_size, len(dataset)-train_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "\n",
    "\n",
    "# Adding learning rate scheduling and using it in training loop\n",
    "# Gives smoother scheduling compared to StepLR\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "start_time = timer()\n",
    "model_results = train(model, train_dataloader, test_dataloader, loss_fn, optimizer, scheduler=scheduler, epochs=NUM_EPOCHS)\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "venv_dir = Path(sys.prefix)\n",
    "project_root = venv_dir.parent\n",
    "models_path = project_root/\"trained_models\"\n",
    "\n",
    "torch.save(model.state_dict(), models_path/\"resnet18_pizzasushisteak.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
