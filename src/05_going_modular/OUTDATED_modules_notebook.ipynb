{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a85198a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../modules/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../modules/data_setup.py\n",
    "\"\"\"\n",
    "Module for creating PyTorch DataLoaders for image classification data\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "NUM_WORKERS = os.cpu_count() or 1\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: Path,\n",
    "    test_dir: Path,\n",
    "    transform: transforms.Compose,\n",
    "    batch_size: int,\n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "    \"\"\"Creates training and testing DataLoaders\n",
    "\n",
    "    Takes in a training directory and a test directory, and turns them into PyTorch datasets and then into PyTorch DataLoaders\n",
    "\n",
    "    Args:\n",
    "        train_dir: Path to training directory\n",
    "        test_dir: Path to testing directory\n",
    "        transform: torchvision transforms to beperformed on training and testing data\n",
    "        batch_size: Batch size for the DataLoaders\n",
    "        num_workers: Number of workers for the DataLoaders\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (train_dataloader, test_dataloader, class_names), where class_names is a list of the target classes.\n",
    "    \"\"\"\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    class_names = train_dataset.classes\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f51ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../modules/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../modules/model_builder.py\n",
    "\"\"\"\n",
    "Module to instantiate a TinyVGG PyTorch model\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"Creates the TinyVGG architecture\n",
    "    \n",
    "    Replicates, with some differences, the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
    "    Original architecture at: https://poloclub.github.io/cnn-explainer/.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: The number of input channels\n",
    "        hidden_units: The number of hidden units between layers\n",
    "        output_shape: The number of output units, ofter the number of classes\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2\n",
    "            )\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Using adaptive average to avoid hardcoding input image dimensions in the last linear layer\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_units*7*7, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.classifier(self.conv_block_2(self.conv_block_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4fd203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../modules/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../modules/engine.py\n",
    "\"\"\"\n",
    "Module contains functions to train and test a PyTorch model\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Performs a single training step on a PyTorch model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to be trained\n",
    "        dataloader: DataLoader instance containing training data\n",
    "        loss_fn: Loss function to minimize\n",
    "        optimizer: PyTorch optimizer used to minimize the loss function\n",
    "        device: Target device to compute on\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of training loss and accuracy, in the form (train_loss, train_accuracy).\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (torch.eq(y_pred_class, y)).sum().item()/len(y_pred)\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    return (train_loss, train_acc)\n",
    "\n",
    "def test_step(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Performs a single test step on a PyTorch model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to be tested\n",
    "        dataloader: DataLoader instance containing test data\n",
    "        loss_fn: Loss function to evaluate model\n",
    "        device: Target device to compute on\n",
    "\n",
    "    Returns:\n",
    "        A tuple of testing loss and accuracy, in the form (test_loss, test_accuracy).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            test_acc += (torch.eq(y_pred_class, y)).sum().item()/len(y_pred)\n",
    "\n",
    "        test_loss /= len(dataloader)\n",
    "        test_acc /= len(dataloader)\n",
    "        return (test_loss, test_acc)\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None,\n",
    "    device: torch.device=torch.device(\"cpu\"),\n",
    "    epochs: int=5,\n",
    "    writer: SummaryWriter=None\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"Train and tests a PyTorch model\n",
    "\n",
    "    Calls train_step() and test_steps() functions to train and test a model for a given number of epoches. Supports learning rate scheduling.\n",
    "    Calculates, print and store training and testing metrics throughout for monitoring.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to be trained\n",
    "        train_dataloader: DataLoader instance containing training data\n",
    "        test_dataloader: DataLoader instance containing test data\n",
    "        loss_fn: Loss function to minimize\n",
    "        optimizer: PyTorch optimizer used to minimize the loss function\n",
    "        scheduler: PyTorch learning rate scheduler for the optimizer, not mandatory\n",
    "        device: Target device to compute on (default is \"cpu\")\n",
    "        epochs: Number of epochs for the training (default is 5)\n",
    "        writer: A SummaryWriter can be given to the function to be reasured\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of training and testing loss and training and testing accuracy for each epoch.\n",
    "        In the form: {train_loss: [...],\n",
    "                    train_acc: [...],\n",
    "                    test_loss: [...],\n",
    "                    test_acc: [...]}\n",
    "    \"\"\"\n",
    "    new_writer = writer is None\n",
    "    if(new_writer):\n",
    "        writer = SummaryWriter()\n",
    "    \n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        if(scheduler is not None):\n",
    "            scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        writer.add_scalars(\n",
    "            main_tag=\"Loss\",\n",
    "            tag_scalar_dict={\"train_loss\": train_loss, \"test_loss\": test_loss},\n",
    "            global_step=epoch\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            main_tag=\"Accuracy\",\n",
    "            tag_scalar_dict={\"train_acc\": train_acc, \"test_acc\": test_acc},\n",
    "            global_step=epoch\n",
    "        )\n",
    "\n",
    "        writer.add_graph(model=model, input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "    \n",
    "    if new_writer:\n",
    "        writer.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098dc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../modules/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../modules/utils.py\n",
    "\"\"\"\n",
    "Utility module that helps with training, saving and loading a PyTorch model\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "def save_model(\n",
    "    model: torch.nn.Module,\n",
    "    target_dir_path: Path,\n",
    "    model_name: str\n",
    "):\n",
    "    \"\"\"Saves a PyTorch model to a target directory\n",
    "\"%Y-%m-%d\"\n",
    "    Args:\n",
    "        model: PyTorch model to save\n",
    "        target_dir: Directory where the model will be saved\n",
    "        model_name: Name under which the model will be saved, should end in \".pth\" of \".pt\" as for naming conventions\n",
    "    \"\"\"\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name doesn't follow naming convention\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    print(f\"Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(), f=model_save_path)\n",
    "\n",
    "def download_data(\n",
    "    source: str,\n",
    "    destination: Path,\n",
    "    remove_source: bool=True\n",
    ") -> Path:\n",
    "    \"\"\"Downloads a zipped dataset from source and unzips to destination\n",
    "    \n",
    "    Args:\n",
    "        source: A link to a zip archive containing data\n",
    "        destination: The directory the datasets will be unzipped to\n",
    "        remove_source: Wheter to remove the source after extracting\n",
    "        \n",
    "    Returns:\n",
    "        pathlib.Path to the downloaded data directory\n",
    "    \"\"\"\n",
    "    venv_dir = Path(sys.prefix)\n",
    "    project_root = venv_dir.parent\n",
    "    data_path = project_root/\"data\"\n",
    "    image_path = data_path/destination\n",
    "\n",
    "    if(image_path.is_dir() is False):\n",
    "        image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        target_file = Path(source).name\n",
    "        with open(data_path/target_file, \"wb\") as f:\n",
    "            request = requests.get(source)\n",
    "            f.write(request.content)\n",
    "\n",
    "        with zipfile.ZipFile(data_path/target_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(image_path)\n",
    "\n",
    "        if remove_source:\n",
    "            os.remove(data_path/target_file)\n",
    "    \n",
    "    return image_path\n",
    "\n",
    "def create_writer(\n",
    "    experiment_name: str,\n",
    "    model_name: str,\n",
    "    extra: str=None\n",
    ") -> SummaryWriter():\n",
    "    \"\"\"Creates a SummaryWriter() instance that saves to a specific directory\n",
    "    \n",
    "    The logs directory is a combination of runs/timestamp/experiment_name/extra\n",
    "    Where timestamp is the current date in YYYY-MM-DD format\n",
    "    \n",
    "    Args:\n",
    "    - experiment_name: Name of the experiment\n",
    "    - model_name: Name of the model\n",
    "    - extra: Extra informations, defaults to None\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "    if extra:\n",
    "        log_dir = os.path.join(log_dir, extra)\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d906a7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../modules/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../modules/train.py\n",
    "\"\"\"\n",
    "Trains and saves a PyTorch image classification model using device-agnostic code\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "import data_setup, engine, model_builder, utils\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Trains and saves a PyTorch image classification model using device-agnostic code\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=5, help=\"Number of epoches for the training and testing\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Dimension of every batch\")\n",
    "parser.add_argument(\"--hidden_units\", type=int, default=10, help=\"Hidden units for every model layer, except for input and output\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001, help=\"Starting learning rate for the training\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "venv_dir = Path(sys.prefix)\n",
    "project_root = venv_dir.parent\n",
    "data_dir = project_root/\"data\"\n",
    "train_dir = data_dir/\"pizza_steak_sushi/train\"\n",
    "test_dir = data_dir/\"pizza_steak_sushi/test\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataloader, test_dataloder, class_names = data_setup.create_dataloaders(train_dir, test_dir, data_transform, args.batch_size)\n",
    "\n",
    "model = model_builder.TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=args.hidden_units,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=args.num_epochs)\n",
    "\n",
    "engine.train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloder,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    args.num_epochs\n",
    ")\n",
    "\n",
    "models_dir = project_root/\"trained_models\"\n",
    "utils.save_model(model, models_dir, \"tinyvgg_from_cmd.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
