{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9efb652",
   "metadata": {},
   "source": [
    "##### Paper Replicating\n",
    "Paper replicating is the practice of reproducing models and techniques explained in new papers to grasp new technologies. Resources can be found, for example, at **arXiv**, **AK Twitter**, **Papers with code** and **lucidrains' vit-pitorch GitHub repository**\n",
    "\n",
    "##### Sequential Data\n",
    "\n",
    "Many tasks involve sequential data, for example text, video and audio. When training on this types of data, a model should be able to understand relationships between different elements in the sequence. When working with text, the first models able to do so  where RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks), but both had some flaws. LSTMs fixed the main problem with RNNs, not being able to remember connections between fare apart, but still processed data sequentially. So training was slow and hard to parallelize.\n",
    "\n",
    "\n",
    "##### Transformers\n",
    "Google introduced transformers in 2017, in the paper *Attention is All You Need*. A Transformer architecture is considered to be any neural network that uses the **attention mechanism** as its primary learning layer. This NNs are able to process entire sentences simultaneously, by understanding relationships between works.\n",
    "\n",
    "\n",
    "##### Vision Transformers\n",
    "If an image is split in patches, then flattened into a vector, then it becomes sequential data. Vision Transformers, ViT for short, process images in this new way, leveraging Attention and feeding the sequence into a Transformer Encoder. Vanilla ViT is the original ViT, introduced in 2020 in the paper *An Image is Worth 16x16 Words*."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
