{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0564648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "gastrovision_big_demo_path = Path(\"demos/gastrovision_big\")\n",
    "\n",
    "if gastrovision_big_demo_path.exists():\n",
    "    shutil.rmtree(gastrovision_big_demo_path)\n",
    "\n",
    "gastrovision_big_demo_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gastrovision_big_examples_path = gastrovision_big_demo_path / \"examples\"\n",
    "gastrovision_big_examples_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9631eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "def create_effnet2_model(num_classes: int=3):\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=None)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes)\n",
    "    )\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "862114bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/hot_and_sour_soup/1277295.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/beet_salad/1279787.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/waffles/1371262.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/caesar_salad/2970338.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/club_sandwich/3620923.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/beef_carpaccio/3574772.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/greek_salad/1694440.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/lobster_roll_sandwich/2192375.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/miso_soup/168469.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/oysters/2794852.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/edamame/1695174.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/baklava/1174313.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/gnocchi/3607261.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/crab_cakes/408582.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/lobster_roll_sandwich/1946264.jpg')]\n",
      "[{'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/hot_and_sour_soup/1277295.jpg'), 'class_name': 'hot_and_sour_soup', 'pred_prob': tensor([[1.6510e-06, 4.3293e-05, 2.5919e-06, 2.2570e-05, 2.5927e-06, 5.1267e-06,\n",
      "         9.9072e-07, 2.5288e-04, 4.9495e-06, 1.0751e-05, 8.5973e-07, 1.7524e-06,\n",
      "         7.6419e-07, 1.1773e-05, 7.7468e-06, 1.9596e-06, 2.4218e-06, 1.1741e-06,\n",
      "         4.7956e-04, 2.1014e-05, 9.3839e-06, 2.3470e-06, 1.7141e-06, 7.7264e-06,\n",
      "         1.5714e-05, 6.9560e-07, 1.2329e-05, 1.6081e-04, 1.5130e-06, 5.6066e-08,\n",
      "         1.7130e-07, 5.4949e-06, 1.2816e-05, 1.1274e-05, 2.8082e-06, 3.8420e-04,\n",
      "         2.6311e-05, 5.4964e-06, 3.2390e-05, 4.3802e-06, 1.2892e-05, 3.9797e-03,\n",
      "         1.6184e-04, 2.7480e-05, 3.4320e-05, 2.7511e-06, 1.6888e-06, 1.3447e-04,\n",
      "         3.9744e-06, 8.9079e-06, 2.9449e-05, 4.9657e-06, 1.3386e-05, 2.4449e-06,\n",
      "         9.9049e-01, 8.5330e-07, 8.5226e-05, 4.3306e-05, 3.4917e-07, 1.2378e-04,\n",
      "         7.9860e-06, 1.9458e-06, 1.4254e-05, 2.4854e-07, 1.2998e-04, 7.0664e-04,\n",
      "         8.9493e-06, 1.5030e-05, 2.9698e-06, 7.9991e-06, 1.3680e-04, 1.9919e-04,\n",
      "         2.3022e-06, 1.1268e-05, 4.9960e-05, 4.5991e-04, 6.8005e-06, 1.5410e-05,\n",
      "         2.3460e-05, 6.7526e-06, 3.5153e-05, 3.3729e-04, 3.8998e-05, 2.4823e-07,\n",
      "         8.3023e-05, 5.5230e-05, 7.0906e-06, 4.4152e-06, 1.8406e-04, 4.4120e-05,\n",
      "         4.6542e-04, 6.2560e-06, 6.6907e-05, 3.9485e-05, 9.7598e-08, 7.0536e-07,\n",
      "         1.7988e-05, 6.2923e-05, 2.7914e-06, 1.5507e-06, 3.5618e-06]]), 'pred_class': 'hot_and_sour_soup', 'time_for_pred': 0.0767, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/beet_salad/1279787.jpg'), 'class_name': 'beet_salad', 'pred_prob': tensor([[9.0888e-04, 1.7568e-03, 7.7048e-03, 1.1024e-02, 1.4530e-02, 4.2370e-01,\n",
      "         7.8211e-04, 4.3373e-04, 1.0169e-03, 1.0695e-03, 2.8636e-03, 2.0221e-03,\n",
      "         9.5091e-04, 1.5426e-02, 1.8378e-02, 2.0241e-02, 2.6696e-02, 3.3012e-02,\n",
      "         4.5576e-03, 4.1381e-03, 2.8938e-03, 2.2879e-02, 9.0649e-03, 1.1500e-04,\n",
      "         5.4145e-04, 2.8991e-03, 6.2555e-03, 7.8081e-04, 7.5176e-03, 2.8317e-04,\n",
      "         1.3882e-02, 2.9245e-05, 2.8653e-04, 2.8641e-03, 1.3978e-04, 2.1969e-04,\n",
      "         1.4199e-03, 6.4278e-03, 3.5365e-03, 1.2998e-01, 1.7140e-04, 2.6254e-05,\n",
      "         7.2814e-04, 3.3554e-03, 2.3365e-03, 2.6242e-03, 3.3128e-04, 2.4187e-03,\n",
      "         8.8190e-04, 5.1882e-04, 9.5860e-04, 5.1588e-04, 6.0142e-04, 1.3487e-03,\n",
      "         1.6266e-04, 2.7368e-05, 3.4914e-03, 2.9696e-03, 1.2840e-02, 3.3259e-03,\n",
      "         4.7292e-03, 2.1158e-04, 3.5499e-04, 1.4992e-03, 5.3355e-03, 2.4195e-03,\n",
      "         3.5520e-03, 1.3734e-03, 1.8288e-05, 3.7481e-03, 9.3318e-05, 2.6570e-04,\n",
      "         1.1234e-03, 1.7445e-02, 4.6977e-03, 3.9146e-04, 2.8456e-03, 1.9767e-03,\n",
      "         2.1092e-04, 9.4562e-04, 6.6178e-05, 7.2527e-04, 4.3174e-03, 3.0574e-03,\n",
      "         1.0046e-03, 5.8501e-03, 1.5476e-02, 1.5579e-03, 1.0454e-03, 1.3032e-04,\n",
      "         4.1152e-04, 3.2524e-04, 3.6476e-03, 3.3852e-03, 1.8606e-03, 3.7721e-03,\n",
      "         5.2522e-04, 2.4002e-05, 7.5209e-03, 4.9103e-02, 9.2343e-05]]), 'pred_class': 'beet_salad', 'time_for_pred': 0.0381, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/waffles/1371262.jpg'), 'class_name': 'waffles', 'pred_prob': tensor([[4.6303e-03, 2.6418e-02, 8.2511e-03, 1.7386e-03, 1.1486e-02, 5.3410e-03,\n",
      "         2.7954e-03, 1.6387e-03, 1.5725e-03, 9.5619e-03, 1.2119e-03, 2.2187e-03,\n",
      "         4.2925e-03, 1.5507e-03, 1.6377e-02, 7.5854e-03, 3.4301e-03, 7.7366e-03,\n",
      "         3.3890e-03, 1.9087e-02, 2.3971e-03, 6.0337e-03, 3.0149e-03, 2.8316e-03,\n",
      "         2.6664e-02, 1.2321e-02, 6.9062e-03, 3.6477e-03, 9.0717e-03, 2.2492e-03,\n",
      "         2.9180e-03, 3.0705e-03, 6.8092e-03, 5.6926e-03, 2.8335e-03, 5.5387e-03,\n",
      "         4.5800e-03, 6.4881e-03, 1.1901e-02, 3.9430e-03, 6.1142e-02, 1.3207e-02,\n",
      "         3.0488e-03, 9.1564e-03, 5.1610e-03, 1.4696e-03, 8.3939e-03, 1.2071e-02,\n",
      "         4.1764e-03, 4.9558e-02, 4.7528e-03, 3.7870e-03, 6.3777e-03, 1.9522e-02,\n",
      "         4.0866e-02, 1.0136e-02, 4.1380e-03, 2.9886e-03, 1.8696e-03, 2.8466e-02,\n",
      "         1.4939e-02, 1.1241e-02, 2.1885e-02, 2.3294e-03, 2.8346e-04, 3.2046e-03,\n",
      "         4.8916e-04, 8.5385e-03, 6.1558e-03, 1.4992e-03, 6.0053e-03, 2.3874e-03,\n",
      "         2.3954e-03, 5.0680e-03, 1.5694e-03, 1.4007e-04, 1.3020e-02, 7.9736e-03,\n",
      "         1.8553e-02, 8.7160e-03, 1.4327e-02, 7.5942e-04, 2.4859e-03, 6.0201e-03,\n",
      "         1.7918e-03, 6.8980e-03, 2.3088e-03, 1.4723e-03, 2.4365e-03, 8.8885e-04,\n",
      "         1.5860e-02, 2.6217e-03, 1.4853e-02, 1.3694e-02, 1.2177e-03, 5.9332e-03,\n",
      "         3.9423e-04, 8.3471e-04, 1.4006e-03, 4.3768e-03, 2.0352e-01]]), 'pred_class': 'waffles', 'time_for_pred': 0.0369, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/caesar_salad/2970338.jpg'), 'class_name': 'caesar_salad', 'pred_prob': tensor([[1.9206e-03, 1.6414e-01, 5.3960e-03, 1.5708e-03, 5.6564e-03, 2.5758e-02,\n",
      "         7.7146e-04, 1.1152e-02, 1.2093e-04, 7.7508e-03, 3.9881e-03, 1.4653e-01,\n",
      "         3.8831e-04, 5.7298e-03, 7.8367e-04, 1.4746e-03, 4.0329e-03, 1.4700e-03,\n",
      "         6.1664e-03, 1.2464e-02, 7.5697e-03, 2.7063e-03, 9.2907e-05, 7.6016e-04,\n",
      "         4.0365e-03, 1.2549e-02, 9.9791e-03, 8.3649e-04, 7.5581e-03, 8.9511e-04,\n",
      "         3.0138e-04, 1.4815e-03, 1.2671e-03, 5.8078e-03, 4.9052e-03, 4.7704e-03,\n",
      "         9.6502e-03, 1.0576e-02, 3.4559e-03, 1.1688e-03, 4.2307e-04, 1.5195e-03,\n",
      "         1.8431e-03, 4.3845e-04, 1.5013e-02, 2.7046e-04, 1.0995e-03, 7.6886e-04,\n",
      "         1.0011e-03, 4.6839e-03, 1.7295e-02, 5.1799e-02, 4.0086e-03, 1.0114e-01,\n",
      "         4.7479e-04, 1.4088e-03, 2.1921e-02, 1.1825e-03, 2.3914e-04, 1.3413e-03,\n",
      "         2.3826e-04, 1.0609e-02, 6.4120e-04, 2.6694e-04, 3.0601e-04, 1.2163e-02,\n",
      "         1.2905e-03, 4.1879e-02, 8.9335e-04, 7.5548e-04, 9.1542e-03, 3.3374e-03,\n",
      "         1.9782e-04, 2.1435e-04, 4.6431e-02, 2.1243e-03, 5.5258e-04, 1.1550e-02,\n",
      "         2.4473e-03, 2.2699e-02, 8.8129e-03, 4.4964e-04, 1.8625e-03, 2.7522e-04,\n",
      "         5.0747e-03, 9.1201e-03, 9.2343e-03, 3.6405e-03, 1.8224e-02, 2.1181e-03,\n",
      "         3.7545e-04, 1.1738e-03, 4.8489e-03, 2.1459e-02, 8.4446e-04, 1.0260e-02,\n",
      "         1.3113e-03, 1.4651e-04, 1.9417e-03, 5.0884e-03, 4.8602e-04]]), 'pred_class': 'baby_back_ribs', 'time_for_pred': 0.0365, 'correct': False}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/club_sandwich/3620923.jpg'), 'class_name': 'club_sandwich', 'pred_prob': tensor([[1.5757e-03, 4.9131e-05, 1.1312e-03, 7.1512e-03, 7.3570e-03, 6.1498e-03,\n",
      "         6.0250e-04, 8.0292e-04, 1.3819e-03, 1.2705e-03, 4.1263e-02, 2.6965e-02,\n",
      "         4.9938e-04, 3.6444e-02, 1.7002e-03, 1.1500e-02, 3.3779e-04, 1.9063e-03,\n",
      "         1.1776e-03, 1.0877e-02, 2.0292e-04, 4.7756e-04, 5.8931e-04, 3.6041e-04,\n",
      "         1.3243e-03, 3.0429e-01, 2.2308e-03, 9.9652e-04, 1.3970e-03, 1.7696e-04,\n",
      "         1.1102e-02, 2.0251e-04, 9.1756e-05, 4.2255e-03, 3.7730e-04, 1.1001e-03,\n",
      "         1.3288e-02, 4.3516e-03, 5.2524e-04, 4.6633e-04, 1.0651e-03, 1.8202e-04,\n",
      "         2.4715e-04, 1.2613e-03, 9.1281e-03, 1.8285e-03, 1.8746e-03, 3.8396e-03,\n",
      "         4.7415e-02, 6.2705e-03, 2.3282e-03, 5.1369e-02, 1.1812e-04, 7.8952e-03,\n",
      "         2.8068e-04, 2.8634e-03, 2.1095e-02, 2.0140e-02, 1.9326e-02, 3.1142e-02,\n",
      "         3.1364e-03, 4.1126e-02, 8.4866e-04, 1.0471e-03, 5.1960e-04, 1.2749e-04,\n",
      "         2.9370e-03, 1.2486e-03, 1.6969e-04, 8.7231e-04, 3.3827e-03, 3.3192e-03,\n",
      "         3.9831e-04, 1.4216e-03, 2.5741e-04, 8.8338e-04, 4.0024e-03, 3.5867e-04,\n",
      "         1.6571e-04, 4.7858e-03, 1.0829e-03, 6.7208e-04, 2.4508e-02, 8.6543e-04,\n",
      "         3.0373e-02, 1.0536e-03, 3.5336e-02, 5.2179e-03, 2.3756e-03, 3.8155e-03,\n",
      "         3.8816e-04, 1.3527e-03, 9.7509e-04, 1.4500e-03, 1.4699e-02, 1.7888e-02,\n",
      "         8.6536e-03, 1.6471e-03, 3.7784e-03, 3.5079e-02, 1.8900e-04]]), 'pred_class': 'club_sandwich', 'time_for_pred': 0.0346, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/beef_carpaccio/3574772.jpg'), 'class_name': 'beef_carpaccio', 'pred_prob': tensor([[6.4123e-04, 1.2564e-03, 6.3890e-04, 6.1700e-02, 1.7774e-02, 4.3385e-02,\n",
      "         2.8004e-03, 1.0124e-02, 3.1241e-03, 4.8663e-03, 1.2158e-02, 1.8063e-02,\n",
      "         5.2566e-03, 2.0579e-01, 5.8974e-03, 1.8011e-02, 6.8373e-03, 1.0998e-02,\n",
      "         4.6705e-03, 9.5800e-03, 1.9990e-03, 4.7929e-03, 1.6638e-03, 2.8736e-04,\n",
      "         6.2740e-03, 9.1979e-03, 2.8339e-03, 4.3659e-04, 4.1059e-03, 3.1841e-03,\n",
      "         1.2873e-02, 4.9909e-03, 8.2923e-04, 2.3677e-03, 1.6622e-02, 3.3278e-03,\n",
      "         9.8872e-03, 2.9372e-03, 4.9359e-04, 3.1438e-03, 2.4257e-03, 3.3814e-03,\n",
      "         1.6421e-03, 2.0999e-03, 8.4211e-03, 4.3738e-03, 5.6175e-03, 3.6787e-03,\n",
      "         2.5814e-02, 1.7341e-03, 7.1466e-04, 5.6197e-04, 1.9036e-04, 1.9923e-03,\n",
      "         9.1420e-04, 1.1724e-02, 1.2900e-02, 4.7454e-03, 3.3309e-03, 3.0560e-03,\n",
      "         1.2099e-03, 2.3769e-03, 4.3117e-03, 2.2792e-03, 3.8256e-03, 4.4616e-03,\n",
      "         6.9613e-02, 2.8994e-03, 6.9830e-04, 6.6974e-03, 6.3267e-03, 5.9184e-03,\n",
      "         5.4736e-03, 2.6387e-02, 4.4239e-03, 3.7472e-03, 5.4421e-02, 1.9261e-03,\n",
      "         8.7689e-03, 4.1632e-03, 4.4829e-03, 4.1493e-03, 2.9648e-03, 6.3242e-03,\n",
      "         4.9526e-03, 4.7835e-04, 8.1552e-03, 3.4311e-02, 2.9707e-03, 3.4818e-03,\n",
      "         9.7131e-04, 6.8746e-03, 9.6659e-04, 2.3645e-03, 7.3396e-03, 9.3968e-03,\n",
      "         9.9213e-03, 5.1875e-03, 1.2561e-02, 1.3074e-02, 4.0049e-03]]), 'pred_class': 'caprese_salad', 'time_for_pred': 0.0358, 'correct': False}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/greek_salad/1694440.jpg'), 'class_name': 'greek_salad', 'pred_prob': tensor([[2.0920e-05, 1.3016e-04, 4.1905e-06, 3.9087e-02, 8.1847e-03, 1.3673e-01,\n",
      "         7.2681e-04, 9.5082e-04, 6.4059e-05, 3.2727e-05, 2.3665e-02, 3.5721e-03,\n",
      "         1.3264e-05, 6.9836e-02, 2.6551e-05, 4.8901e-03, 9.3726e-05, 2.1544e-04,\n",
      "         1.8210e-04, 1.7078e-04, 1.2466e-04, 2.1211e-05, 5.5256e-05, 4.1485e-05,\n",
      "         3.2771e-05, 4.9923e-04, 1.5119e-03, 2.2106e-04, 2.8959e-04, 1.1236e-04,\n",
      "         9.7868e-04, 1.2752e-03, 1.3970e-04, 4.6051e-04, 2.4470e-04, 7.1618e-04,\n",
      "         1.1938e-03, 1.4054e-04, 1.4511e-05, 6.3437e-03, 2.5241e-04, 1.4880e-05,\n",
      "         1.6977e-05, 5.2189e-04, 4.6597e-04, 1.5452e-05, 1.6453e-05, 2.5695e-04,\n",
      "         6.1899e-01, 1.2585e-04, 4.0172e-04, 3.2306e-04, 1.8559e-04, 1.7806e-02,\n",
      "         2.7200e-05, 9.2586e-04, 4.8807e-03, 1.3541e-04, 4.3618e-05, 1.5039e-04,\n",
      "         3.6080e-05, 2.0476e-04, 7.1102e-05, 3.2863e-04, 7.4276e-05, 1.0941e-05,\n",
      "         6.2054e-04, 3.2013e-05, 1.4136e-04, 1.8451e-04, 7.5022e-05, 1.2166e-05,\n",
      "         1.7053e-04, 2.3257e-03, 6.1757e-04, 1.9156e-04, 1.3943e-03, 1.2051e-04,\n",
      "         4.9757e-05, 4.9493e-05, 3.1210e-04, 8.8355e-05, 1.5087e-04, 4.6645e-05,\n",
      "         3.9505e-04, 9.2106e-05, 8.1599e-04, 9.0990e-03, 1.4371e-02, 7.3222e-04,\n",
      "         2.3034e-04, 1.0446e-04, 4.3471e-04, 3.6088e-04, 3.3354e-04, 4.9951e-04,\n",
      "         1.5936e-03, 6.2016e-03, 7.3324e-06, 8.8340e-03, 1.5154e-05]]), 'pred_class': 'greek_salad', 'time_for_pred': 0.0351, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/lobster_roll_sandwich/2192375.jpg'), 'class_name': 'lobster_roll_sandwich', 'pred_prob': tensor([[7.9359e-05, 8.8964e-04, 6.6321e-04, 7.0462e-03, 8.5278e-04, 8.9802e-03,\n",
      "         1.3068e-03, 1.5562e-04, 1.3288e-04, 4.1343e-03, 8.7448e-03, 2.7028e-03,\n",
      "         2.6168e-03, 1.4934e-02, 3.2760e-04, 4.7847e-02, 1.4131e-02, 7.2956e-05,\n",
      "         5.6870e-04, 1.0028e-03, 1.5041e-04, 1.4319e-04, 1.3412e-04, 4.5319e-03,\n",
      "         7.5116e-04, 1.0322e-02, 8.0268e-04, 4.3741e-04, 5.2841e-03, 1.9336e-04,\n",
      "         2.5126e-02, 5.7057e-03, 5.0822e-04, 4.8217e-05, 1.1288e-03, 2.0687e-03,\n",
      "         2.9720e-03, 4.4602e-04, 1.1009e-02, 4.1250e-04, 9.5567e-03, 9.0032e-05,\n",
      "         1.4101e-03, 2.2781e-02, 6.6451e-04, 1.4016e-04, 1.1542e-02, 3.6525e-04,\n",
      "         6.0996e-03, 6.3914e-03, 4.5289e-04, 2.8645e-04, 4.6391e-04, 1.7432e-02,\n",
      "         1.6497e-05, 4.2593e-01, 1.6059e-03, 2.1949e-04, 1.5223e-04, 6.4711e-04,\n",
      "         2.1256e-03, 1.7994e-01, 7.7816e-04, 3.9566e-04, 2.1546e-04, 2.6369e-04,\n",
      "         6.9129e-04, 1.2993e-03, 4.3423e-03, 4.2678e-03, 2.4395e-04, 3.9181e-04,\n",
      "         9.8846e-05, 9.1738e-05, 2.9542e-04, 2.8872e-05, 7.0444e-04, 2.0847e-04,\n",
      "         3.7997e-03, 9.9520e-05, 2.4468e-02, 6.0180e-05, 1.0364e-04, 2.6675e-05,\n",
      "         6.6010e-05, 1.2777e-04, 1.9886e-04, 1.8693e-04, 1.8035e-04, 2.0814e-04,\n",
      "         8.8641e-05, 1.4470e-04, 5.9157e-03, 1.0417e-03, 1.8640e-05, 2.4802e-03,\n",
      "         3.7497e-02, 2.9451e-02, 9.7329e-06, 4.5575e-04, 1.3751e-03]]), 'pred_class': 'hot_dog', 'time_for_pred': 0.0464, 'correct': False}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/miso_soup/168469.jpg'), 'class_name': 'miso_soup', 'pred_prob': tensor([[2.8610e-03, 5.4418e-04, 3.5675e-04, 3.3715e-04, 1.0248e-02, 3.2145e-03,\n",
      "         8.4248e-04, 3.3191e-03, 5.4619e-04, 1.8710e-03, 1.9604e-04, 1.7117e-03,\n",
      "         2.5474e-03, 2.3975e-04, 5.2169e-03, 3.4141e-04, 3.1661e-02, 4.3867e-03,\n",
      "         5.5492e-04, 7.2399e-04, 2.9617e-03, 1.7341e-03, 2.9782e-03, 1.9660e-03,\n",
      "         3.4098e-03, 4.8775e-04, 2.4175e-03, 3.8255e-03, 2.2446e-03, 3.5303e-03,\n",
      "         3.9297e-03, 1.8729e-03, 2.9566e-03, 3.7890e-03, 3.0904e-03, 8.1867e-04,\n",
      "         2.2830e-03, 1.2551e-03, 2.0299e-03, 6.9312e-03, 1.6205e-03, 8.1466e-04,\n",
      "         1.2101e-03, 5.4525e-03, 9.5093e-03, 2.7957e-02, 1.8455e-03, 2.6276e-03,\n",
      "         1.5998e-03, 4.5903e-03, 4.2720e-04, 9.3585e-04, 2.4089e-03, 2.0004e-04,\n",
      "         3.8556e-02, 4.4388e-03, 9.1952e-04, 3.7232e-03, 1.8410e-03, 7.1741e-04,\n",
      "         3.8294e-02, 4.5936e-04, 2.2117e-03, 9.7135e-04, 6.5345e-01, 1.1289e-03,\n",
      "         1.6797e-03, 1.3525e-03, 3.0373e-04, 3.0880e-03, 3.3508e-04, 6.4848e-03,\n",
      "         4.7817e-03, 8.0080e-03, 2.0077e-03, 2.0115e-03, 1.2273e-03, 6.2190e-04,\n",
      "         1.9598e-03, 1.6605e-04, 1.2036e-04, 3.0812e-03, 9.8511e-04, 1.4978e-03,\n",
      "         1.8524e-03, 4.5991e-03, 9.1884e-04, 3.3766e-03, 3.1907e-04, 1.9457e-03,\n",
      "         2.0845e-03, 1.1836e-04, 2.4022e-03, 6.7619e-04, 2.6687e-04, 7.8661e-04,\n",
      "         1.2904e-04, 3.3299e-03, 1.6961e-03, 1.1773e-03, 4.6340e-04]]), 'pred_class': 'miso_soup', 'time_for_pred': 0.0686, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/oysters/2794852.jpg'), 'class_name': 'oysters', 'pred_prob': tensor([[9.0375e-04, 4.1418e-03, 5.1232e-04, 7.3997e-04, 9.8386e-04, 4.4279e-04,\n",
      "         3.1213e-03, 1.7278e-03, 7.1073e-03, 2.8825e-03, 6.8599e-04, 3.9831e-04,\n",
      "         1.6439e-02, 3.3576e-04, 1.7668e-05, 2.4957e-04, 3.0307e-03, 2.3299e-03,\n",
      "         2.9370e-03, 5.8272e-04, 5.3082e-04, 3.2550e-04, 1.5792e-03, 6.6634e-04,\n",
      "         8.3515e-03, 3.4982e-04, 1.6430e-04, 3.5318e-03, 1.7799e-04, 1.1565e-04,\n",
      "         3.7096e-05, 2.0662e-03, 1.8629e-03, 1.2888e-03, 5.5509e-05, 6.1040e-04,\n",
      "         2.1388e-03, 5.2728e-04, 4.8807e-05, 5.1908e-04, 3.0317e-04, 3.5979e-03,\n",
      "         5.6735e-03, 5.4372e-04, 6.7314e-03, 2.1185e-02, 1.1579e-04, 9.4885e-04,\n",
      "         7.2754e-05, 3.0981e-04, 1.1779e-04, 5.9419e-05, 2.0669e-03, 1.5807e-04,\n",
      "         2.4743e-02, 7.7648e-05, 2.6304e-03, 2.9232e-03, 8.5897e-03, 2.5179e-04,\n",
      "         1.1038e-02, 5.1847e-05, 1.5441e-04, 8.1800e-05, 3.5113e-02, 2.0025e-03,\n",
      "         1.3899e-04, 6.1649e-04, 9.5376e-05, 7.4132e-01, 7.9441e-04, 1.6039e-03,\n",
      "         4.1070e-04, 3.0563e-04, 7.3770e-03, 4.1754e-04, 1.6679e-04, 7.0303e-03,\n",
      "         4.9955e-04, 1.0943e-02, 1.3535e-03, 5.8691e-03, 3.1282e-04, 3.3463e-05,\n",
      "         9.8888e-04, 2.5770e-04, 1.4658e-04, 3.7132e-04, 1.1201e-04, 1.2275e-03,\n",
      "         1.5814e-04, 1.1676e-04, 2.9423e-03, 3.7436e-03, 4.0882e-04, 3.8775e-04,\n",
      "         1.7748e-04, 1.8286e-03, 3.1713e-03, 8.0033e-05, 5.3645e-04]]), 'pred_class': 'oysters', 'time_for_pred': 0.0363, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/edamame/1695174.jpg'), 'class_name': 'edamame', 'pred_prob': tensor([[1.3382e-04, 1.2331e-04, 5.6463e-06, 1.7265e-04, 9.2591e-04, 2.4999e-04,\n",
      "         6.9359e-05, 1.3073e-04, 7.0081e-05, 7.1065e-05, 5.6698e-05, 5.7126e-05,\n",
      "         1.0406e-05, 4.4128e-04, 5.1052e-05, 1.3684e-03, 1.5848e-04, 3.9436e-05,\n",
      "         7.0839e-05, 1.7998e-05, 4.0787e-04, 9.7307e-05, 5.8381e-05, 5.7057e-05,\n",
      "         1.6872e-04, 2.4694e-04, 1.0450e-03, 1.1644e-04, 1.4730e-04, 1.8373e-04,\n",
      "         4.3771e-05, 2.5386e-05, 4.6323e-03, 9.6844e-01, 3.4141e-04, 7.1959e-05,\n",
      "         1.5824e-03, 2.2808e-04, 9.0092e-04, 2.9039e-05, 1.3229e-03, 1.2800e-04,\n",
      "         2.5327e-05, 5.1241e-05, 7.8847e-05, 4.0580e-04, 8.7807e-05, 1.7260e-04,\n",
      "         1.2147e-03, 2.8991e-04, 6.0363e-04, 2.3561e-04, 3.1417e-04, 2.1641e-03,\n",
      "         4.8106e-05, 7.1445e-05, 5.9065e-04, 1.7220e-04, 3.5839e-04, 1.5998e-04,\n",
      "         5.2290e-05, 1.1571e-04, 3.0915e-04, 2.6737e-04, 1.5038e-04, 2.9935e-05,\n",
      "         5.7889e-04, 5.0496e-05, 3.4011e-04, 2.2750e-05, 8.4110e-05, 3.4638e-05,\n",
      "         8.3277e-05, 1.2348e-04, 8.3023e-04, 1.2438e-04, 1.2258e-05, 2.2969e-04,\n",
      "         1.8718e-04, 2.7210e-04, 6.4568e-05, 1.0245e-05, 2.7923e-04, 4.0012e-05,\n",
      "         2.5734e-04, 2.2205e-04, 3.1459e-04, 5.1432e-04, 1.6282e-04, 1.0716e-04,\n",
      "         3.8306e-05, 9.3632e-05, 9.8525e-05, 1.2945e-03, 7.0790e-05, 4.1436e-05,\n",
      "         6.5641e-04, 5.5999e-06, 4.0320e-05, 1.8572e-04, 6.5027e-05]]), 'pred_class': 'edamame', 'time_for_pred': 0.0342, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/baklava/1174313.jpg'), 'class_name': 'baklava', 'pred_prob': tensor([[0.0030, 0.0031, 0.0710, 0.0025, 0.0043, 0.0012, 0.0007, 0.0020, 0.0072,\n",
      "         0.0039, 0.0073, 0.0104, 0.0066, 0.0024, 0.0131, 0.0058, 0.0105, 0.0245,\n",
      "         0.0040, 0.0055, 0.0011, 0.0117, 0.0044, 0.0502, 0.0050, 0.0038, 0.0011,\n",
      "         0.0211, 0.0007, 0.0367, 0.0064, 0.0079, 0.0292, 0.0053, 0.0111, 0.0306,\n",
      "         0.0036, 0.0017, 0.0012, 0.0091, 0.0374, 0.0025, 0.0048, 0.0188, 0.0038,\n",
      "         0.0092, 0.0075, 0.0029, 0.0076, 0.0057, 0.0009, 0.0244, 0.0025, 0.0050,\n",
      "         0.0030, 0.0104, 0.0061, 0.0151, 0.0388, 0.0027, 0.0015, 0.0040, 0.0086,\n",
      "         0.0292, 0.0022, 0.0006, 0.0090, 0.0046, 0.0052, 0.0082, 0.0023, 0.0195,\n",
      "         0.0045, 0.0573, 0.0022, 0.0006, 0.0449, 0.0007, 0.0027, 0.0038, 0.0063,\n",
      "         0.0008, 0.0032, 0.0136, 0.0004, 0.0033, 0.0126, 0.0095, 0.0004, 0.0024,\n",
      "         0.0142, 0.0038, 0.0022, 0.0010, 0.0133, 0.0042, 0.0051, 0.0093, 0.0066,\n",
      "         0.0203, 0.0158]]), 'pred_class': 'baklava', 'time_for_pred': 0.0332, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/gnocchi/3607261.jpg'), 'class_name': 'gnocchi', 'pred_prob': tensor([[1.0437e-02, 5.5407e-03, 9.9829e-02, 5.8122e-04, 1.3430e-03, 7.6610e-04,\n",
      "         1.4320e-03, 3.4494e-04, 8.1929e-04, 3.4055e-04, 3.8105e-04, 2.2737e-04,\n",
      "         2.4100e-03, 2.7728e-04, 2.1917e-03, 1.2540e-03, 1.9166e-03, 8.6743e-04,\n",
      "         9.2131e-03, 3.4296e-04, 1.6879e-04, 4.1249e-04, 2.8868e-04, 7.4193e-04,\n",
      "         1.5003e-02, 4.0588e-04, 3.2438e-04, 2.2988e-04, 2.9683e-03, 7.3267e-04,\n",
      "         2.4204e-04, 1.8099e-03, 2.3519e-03, 7.4377e-05, 2.3285e-04, 3.2293e-04,\n",
      "         8.0283e-05, 3.6722e-04, 9.0264e-04, 1.8762e-03, 2.2767e-03, 2.4820e-04,\n",
      "         2.4460e-03, 1.6659e-03, 8.7179e-04, 4.5000e-04, 2.6776e-03, 4.5072e-01,\n",
      "         4.3770e-04, 6.7237e-04, 5.6270e-03, 2.9732e-05, 5.6892e-03, 3.6878e-04,\n",
      "         7.0935e-04, 2.0640e-04, 2.5725e-03, 8.3261e-04, 1.3602e-03, 1.8823e-04,\n",
      "         4.6058e-03, 1.5689e-04, 1.3396e-01, 2.1855e-03, 1.1855e-03, 5.6634e-04,\n",
      "         5.9753e-05, 1.1215e-03, 8.6391e-04, 1.0840e-04, 1.9257e-03, 1.7146e-04,\n",
      "         4.0398e-04, 1.0108e-03, 1.2472e-03, 4.0973e-04, 1.3541e-04, 3.9422e-03,\n",
      "         1.5030e-03, 1.0477e-03, 4.0421e-04, 1.5784e-02, 6.9856e-02, 1.4278e-04,\n",
      "         7.5540e-02, 4.4836e-04, 3.9082e-03, 3.7155e-03, 6.5234e-05, 1.5951e-03,\n",
      "         2.7159e-03, 8.4086e-03, 8.4375e-04, 1.2952e-03, 2.3372e-04, 8.9348e-04,\n",
      "         8.4794e-05, 1.4648e-04, 9.6884e-05, 1.7350e-03, 5.3744e-03]]), 'pred_class': 'gnocchi', 'time_for_pred': 0.0327, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/crab_cakes/408582.jpg'), 'class_name': 'crab_cakes', 'pred_prob': tensor([[1.2599e-03, 3.4854e-02, 4.1131e-03, 1.6400e-03, 6.9543e-03, 9.8567e-03,\n",
      "         1.4007e-04, 8.7367e-03, 1.8895e-04, 1.6041e-02, 3.9183e-03, 1.2446e-02,\n",
      "         2.0212e-03, 4.7780e-04, 7.2357e-03, 3.5915e-03, 1.0709e-03, 1.4402e-03,\n",
      "         1.0297e-01, 4.8783e-03, 1.9962e-03, 2.3847e-03, 1.1986e-04, 1.2743e-03,\n",
      "         8.8439e-04, 3.6556e-03, 1.2789e-01, 3.8573e-04, 2.1367e-03, 1.1003e-03,\n",
      "         6.9495e-03, 1.6001e-03, 4.6490e-03, 3.1606e-04, 3.4976e-03, 5.7260e-04,\n",
      "         1.5459e-02, 3.6713e-03, 2.1239e-02, 6.2439e-04, 1.5533e-03, 1.1813e-04,\n",
      "         5.5460e-03, 4.0700e-03, 5.5580e-02, 1.2462e-04, 7.0643e-04, 1.6835e-03,\n",
      "         3.0383e-04, 4.6654e-02, 5.2662e-03, 5.2534e-03, 1.7494e-02, 7.8130e-03,\n",
      "         6.2095e-04, 1.9786e-02, 1.8114e-02, 7.5200e-04, 1.9419e-03, 1.5405e-02,\n",
      "         2.2685e-04, 3.2721e-03, 3.9253e-03, 2.7565e-04, 9.4170e-05, 1.0202e-05,\n",
      "         2.6358e-02, 4.2513e-02, 8.1747e-03, 1.3105e-04, 3.0814e-02, 2.1221e-03,\n",
      "         9.9082e-04, 3.3970e-05, 2.0514e-03, 3.0387e-05, 1.1599e-03, 3.3412e-04,\n",
      "         3.0812e-03, 1.4318e-03, 8.6876e-03, 2.2116e-04, 2.5988e-03, 1.9256e-03,\n",
      "         7.6107e-04, 6.3972e-03, 1.7562e-03, 2.2694e-03, 3.4051e-02, 5.5659e-04,\n",
      "         5.1761e-03, 8.3621e-03, 2.1778e-02, 5.2768e-03, 5.4337e-05, 2.1704e-02,\n",
      "         7.8405e-03, 7.2152e-02, 3.3042e-04, 3.7645e-02, 4.0008e-04]]), 'pred_class': 'crab_cakes', 'time_for_pred': 0.0336, 'correct': True}, {'image_path': PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/lobster_roll_sandwich/1946264.jpg'), 'class_name': 'lobster_roll_sandwich', 'pred_prob': tensor([[1.3845e-04, 5.4378e-03, 1.4986e-04, 4.0128e-02, 3.1517e-03, 2.4313e-01,\n",
      "         3.2217e-04, 1.4837e-03, 2.1796e-05, 7.6996e-04, 4.3633e-03, 3.8638e-03,\n",
      "         3.5387e-05, 4.1963e-03, 1.1343e-03, 1.0875e-02, 1.7904e-03, 2.5916e-04,\n",
      "         9.1377e-04, 7.3117e-03, 3.5404e-04, 1.1830e-03, 3.4148e-04, 8.0609e-04,\n",
      "         8.9941e-04, 1.9906e-04, 1.5560e-03, 1.5818e-04, 9.0038e-04, 1.2484e-05,\n",
      "         2.3702e-02, 1.2739e-04, 1.3267e-03, 3.6611e-04, 1.1289e-03, 1.9573e-04,\n",
      "         2.7708e-03, 1.4680e-03, 1.2299e-03, 6.2309e-02, 1.3010e-04, 1.0623e-04,\n",
      "         1.5707e-04, 4.2973e-03, 2.9648e-03, 1.0545e-04, 1.5758e-03, 2.1358e-03,\n",
      "         3.9790e-04, 5.5961e-03, 7.2385e-04, 1.7299e-03, 1.2104e-02, 9.6518e-04,\n",
      "         1.9064e-04, 1.4391e-02, 5.1303e-02, 7.1722e-04, 2.3986e-04, 3.9434e-03,\n",
      "         3.0273e-04, 3.3344e-03, 7.2677e-04, 4.6618e-04, 1.7324e-03, 3.8685e-04,\n",
      "         7.8766e-03, 4.3670e-04, 6.8061e-05, 6.4867e-04, 5.8870e-03, 2.5581e-04,\n",
      "         6.6288e-05, 6.9554e-04, 8.1868e-04, 5.6092e-05, 1.4495e-03, 9.2139e-04,\n",
      "         3.8549e-03, 7.7895e-04, 2.4030e-04, 1.2829e-03, 1.5575e-03, 3.2545e-05,\n",
      "         1.5627e-03, 8.1399e-03, 1.0788e-01, 1.2079e-02, 9.3687e-03, 7.8439e-04,\n",
      "         2.2259e-03, 3.0679e-04, 6.5351e-03, 1.0174e-02, 1.3240e-04, 1.3105e-02,\n",
      "         7.8175e-03, 2.1278e-01, 7.6493e-05, 3.8544e-02, 3.1848e-04]]), 'pred_class': 'beet_salad', 'time_for_pred': 0.0325, 'correct': False}]\n",
      "[PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/hot_and_sour_soup/1277295.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/beet_salad/1279787.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/waffles/1371262.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/club_sandwich/3620923.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/greek_salad/1694440.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/miso_soup/168469.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/oysters/2794852.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/edamame/1695174.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/baklava/1174313.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/gnocchi/3607261.jpg'), PosixPath('/home/glauco/Desktop/projects/learningPyTorch/data/food-101/images/crab_cakes/408582.jpg')]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from modules import utils\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "venv_dir = Path(sys.prefix)\n",
    "project_root = venv_dir.parent\n",
    "dataset_path = project_root/\"data\"/\"food-101\"\n",
    "\n",
    "test_data_paths = list(Path(dataset_path/\"images\").glob(\"*/*.jpg\"))\n",
    "example_list = [filepath for filepath in random.sample(test_data_paths, k=15)]\n",
    "print(example_list)\n",
    "\n",
    "\n",
    "with open(dataset_path/\"meta\"/\"classes.txt\", \"r\") as f:\n",
    "    class_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "model, transforms = create_effnet2_model(num_classes=101)\n",
    "weights_path = \"/home/glauco/Desktop/projects/learningPyTorch/trained_models/effnetb2_food101.safetensors\"\n",
    "sd = load_file(weights_path)\n",
    "model.load_state_dict(sd, strict=True)\n",
    "model.eval()\n",
    "\n",
    "results = utils.pred_and_store(\n",
    "    model=model,\n",
    "    paths=example_list,\n",
    "    transforms=transforms,\n",
    "    class_names=class_names,\n",
    "    device=torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n",
    "good_examples = [path for i, path in enumerate(example_list) if results[i][\"correct\"] == True]\n",
    "print(good_examples)\n",
    "\n",
    "\n",
    "\n",
    "for example in good_examples:\n",
    "    destination = gastrovision_big_examples_path / example.name\n",
    "    shutil.copy2(src=example, dst=destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29f96e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/gastrovision_big/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/gastrovision_big/model.py\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "def create_effnet2_model(num_classes: int=3):\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes)\n",
    "    )\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11ec3210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/gastrovision_big/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/gastrovision_big/app.py\n",
    "import os\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "from model import create_effnet2_model\n",
    "from timeit import default_timer as timer\n",
    "from safetensors.torch import load_file, safe_open\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "\n",
    "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
    "\n",
    "effnetb2, effnetb2_transforms = create_effnet2_model(num_classes=101)\n",
    "\n",
    "MODEL_REPO = \"glaucomasi/gastrovision_mini_effnetb2_20percent\"\n",
    "FILENAME   = \"09_pretrained_effnetb2_20_percent.safetensors\"\n",
    "weights_path = \"/home/glauco/Desktop/projects/learningPyTorch/trained_models/effnetb2_food101.safetensors\"\n",
    "\n",
    "# weights_path = hf_hub_download(\n",
    "#     repo_id=MODEL_REPO,\n",
    "#     filename=FILENAME,\n",
    "#     local_dir=\".\",                 # optional: place a real copy into your app folder\n",
    "#     local_dir_use_symlinks=False,  # real file instead of symlink (handy for debug)\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "print(\"== Debug: file existence ==\")\n",
    "print(\"Exists?\", os.path.exists(weights_path))\n",
    "if os.path.exists(weights_path):\n",
    "    print(\"File size (bytes):\", os.path.getsize(weights_path))\n",
    "    # Peek first bytes\n",
    "    with open(weights_path, \"rb\") as f:\n",
    "        head = f.read(200)\n",
    "    print(\"First 200 bytes:\", head[:200])\n",
    "else:\n",
    "    print(\"File not found at:\", weights_path)\n",
    "\n",
    "# Try safetensors open\n",
    "try:\n",
    "    with safe_open(weights_path, framework=\"pt\") as f:\n",
    "        keys = list(f.keys())\n",
    "        print(\"== Debug: safetensors keys sample ==\")\n",
    "        print(keys[:10])\n",
    "except Exception as e:\n",
    "    print(\"== Debug: safetensors load error ==\")\n",
    "    print(repr(e))\n",
    "\n",
    "sd = load_file(weights_path)\n",
    "effnetb2.load_state_dict(sd, strict=True)\n",
    "effnetb2.eval()\n",
    "\n",
    "\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    start_time = timer()\n",
    "\n",
    "    img = effnetb2_transforms(img).unsqueeze(dim=0)\n",
    "\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "\n",
    "    # Gradio's required format: a dictionary with the probability for each class\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    pred_time = round(timer()-start_time, 5)\n",
    "\n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "\n",
    "\n",
    "title = \"Food101 Vision Model\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model achieving '65%' accuracy on Food101 test dataset\"\n",
    "article = \"Created by Glauco Masi\"\n",
    "\n",
    "example_list = [[\"examples/\"+example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
    "        gr.Number(label=\"Prediction time (s)\")\n",
    "    ],\n",
    "    examples=example_list,\n",
    "    title=title,\n",
    "    description=description,\n",
    "    article=article\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd65737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "0.23.0+cu128\n",
      "5.44.1\n",
      "0.6.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gradio\n",
    "import torchvision\n",
    "import safetensors\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(gradio.__version__)\n",
    "print(safetensors.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a909a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/gastrovision_big/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/gastrovision_big/requirements.txt\n",
    "torch==2.8.0\n",
    "torchvision==0.23.0\n",
    "gradio==5.44.1\n",
    "safetensors==0.6.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
